{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"02_prepare\"></a>\n",
    "# 2. Prepare the Data with *pandas* and *NumPy*\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial data, in its raw form, includes variabilities that will become problematic when we process it through a model to find patterns.  The variabilities in the data that we want to mitigate are:\n",
    "\n",
    "* Measurement values have widely varying ranges that require normalization\n",
    "* Not every measurement is taken during every observation, resulting in data gaps\n",
    "* The number of observations per encounter varies widely\n",
    "\n",
    "In this notebook, we will introduce techniques to address these concerns and prepare the data for training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 [**Normalize the Data**](#02_normalize)<br>\n",
    "2.2 [**Fill Data Gaps**](#02_gaps)<br>\n",
    "2.3 [**Pad Variable Length Sequences**](#02_pad)<br>\n",
    "&nbsp; &nbsp; &nbsp;2.3.1 [Exercise: Padded Variable over all Patient Encounters](#02_ex_pad)<br>\n",
    "2.4 [**Save a *NumPy* Array**](#02_save)<br>\n",
    "[**Solutions**](#02_solutions)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import the libraries for the new notebook, this time including _Keras_, our deep learning framework.  We also need to reload the data saved at the end of the previous notebook as `.pkl` (pickle) files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np          \n",
    "import pandas as pd              \n",
    "import matplotlib.pyplot as plt  \n",
    "import random\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "# configure notebook to display plots\n",
    "%matplotlib inline\n",
    "\n",
    "# set up user paths\n",
    "data_dir = '/dli/task/data/hx_series'\n",
    "csv_dir = '/dli/task/csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the DataFrame's loaded in the problem setup\n",
    "X_train=pd.read_pickle('X_train.pkl')\n",
    "y_train=pd.read_pickle('y_train.pkl')\n",
    "X_valid=pd.read_pickle('X_valid.pkl')\n",
    "y_valid=pd.read_pickle('y_valid.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"02_normalize\"></a>\n",
    "## 2.1 Normalize the Data\n",
    "We normalize each observed feature / variable by subtracting its mean and dividing the result by the standard deviation.  \n",
    "\n",
    "Why?\n",
    "\n",
    "We want small variations in one variable to be treated with the same emphasis as HUGE variations of another. Keep in mind that the network just sees a bunch of numbers - it doesn't actually \"know\" anything about predictors, factors, variables, obervations and so on and so forth. Emperically, normalization seems to facilitate training but this kind of normalization is probably not appropriate for multimodal data (or non-Gaussian data in general).\n",
    "\n",
    "Lets take a look at both before and after normalization of a single encounter example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before normalization\n",
    "X_train.loc[8,['Age','Heart rate (bpm)','PulseOximetry','Weight',\n",
    "    'SystolicBP','DiastolicBP','Respiratory rate (bpm)',\n",
    "    'MotorResponse','Capillary refill rate (sec)']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create file path for csv file with metadata about variables\n",
    "metadata = os.path.join(csv_dir, 'ehr_features.csv')\n",
    "\n",
    "# read in variables from csv file (using pandas) since each varable there is tagged with a category\n",
    "variables = pd.read_csv(metadata, index_col=0)\n",
    "\n",
    "# next, select only variables of a particular category for normalization\n",
    "normvars = variables[variables['type'].isin(['Interventions', 'Labs', 'Vitals'])]\n",
    "\n",
    "# finally, iterate over each variable in both training and validation data\n",
    "for vId, dat in normvars.iterrows():\n",
    "    \n",
    "    X_train[vId] = X_train[vId] - dat['mean']\n",
    "    X_valid[vId] = X_valid[vId] - dat['mean']\n",
    "    X_train[vId] = X_train[vId] / (dat['std'] + 1e-12)\n",
    "    X_valid[vId] = X_valid[vId] / (dat['std'] + 1e-12)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a quick look at some the updated data for a single encounter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After normalization\n",
    "X_train.loc[8,['Age','Heart rate (bpm)','PulseOximetry','Weight',\n",
    "    'SystolicBP','DiastolicBP','Respiratory rate (bpm)',\n",
    "    'MotorResponse','Capillary refill rate (sec)']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"02_gaps\"></a>\n",
    "## 2.2 Fill Data Gaps\n",
    "Having normalized the data, we still need to fill in all the data gaps since not every variable was observed during each observation in the patient encounter.  For example, lets take a look at the gaps in a \"Heart rate\" encounter by plotting it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before filling gaps\n",
    "X_train.loc[8, \"Heart rate (bpm)\"].plot()\n",
    "plt.title(\"Normalized Not Filled\")\n",
    "plt.ylabel(\"Heart rate (bpm)\")\n",
    "plt.xlabel(\"Hours since first encounter\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filling in the gaps of missing data is a very active area of research and there is currently no standard practice for time series analysis using deep learning.  For this tutorial, we will simply forward fill existing measurements for each patient, and fill any variable entries with no previous measurement to 0 as illustrated below.\n",
    "\n",
    "<img src=\"images/imputation_diagram.svg\" width=\"800\" height=\"200\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first select variables which will be filled in\n",
    "fillvars = variables[variables['type'].isin(['Vitals', 'Labs'])].index\n",
    "\n",
    "# next forward fill any missing values with more recently observed value\n",
    "X_train[fillvars] = X_train.groupby(level=0)[fillvars].ffill()\n",
    "X_valid[fillvars] = X_valid.groupby(level=0)[fillvars].ffill()\n",
    "\n",
    "# finally, fill in any still missing values with 0 (i.e. values that could not be filled forward)\n",
    "X_train.fillna(value=0, inplace=True)\n",
    "X_valid.fillna(value=0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets have a look at the \"Heart rate\" variable after data normalization and missing values have been filled in.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After filling gaps\n",
    "X_train.loc[8, \"Heart rate (bpm)\"].plot()\n",
    "plt.title(\"Normalized and Filled\")\n",
    "plt.ylabel(\"Heart rate (bpm)\")\n",
    "plt.xlabel(\"Hours since first encounter\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, try dumping the X_train vector to the screen again and you will see that all those NaN values have been filled in with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more thing to note, is that so far we've kept the data set as a _pandas_ DataFrame type.  In the next part, we will change it to a _NumPy_ array.  You can check this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"02_pad\"></a>\n",
    "## 2.3 Pad Variable Length Sequences\n",
    "<img style=\"float: right;\" src=\"images/nobs_hist.png\" width=\"400\">\n",
    "<br>\n",
    "The final data preparation task is to pad every patient encounter so that all encounters have the same number of observations. \n",
    "Note from the histogram, that there are many encounters with less than 100 observation vectors. Therefore, we are going to zero pad each encounter (i.e. insert rows of zeros)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "# max number of sequence length\n",
    "maxlen = 500\n",
    "\n",
    "# get a list of unique patient encounter IDs\n",
    "teId = X_train.index.levels[0]\n",
    "veId = X_valid.index.levels[0]\n",
    "\n",
    "# pad every patient sequence with 0s to be the same length, \n",
    "# then transforms the list of sequences to one numpy array\n",
    "# this is for efficient minibatching and GPU computations \n",
    "X_train = [X_train.loc[patient].values for patient in teId]\n",
    "y_train = [y_train.loc[patient].values for patient in teId]\n",
    "\n",
    "X_train = sequence.pad_sequences(X_train, dtype='float32', maxlen=maxlen, padding='post', truncating='post')\n",
    "y_train = sequence.pad_sequences(y_train, dtype='float32', maxlen=maxlen, padding='post', truncating='post')\n",
    "\n",
    "# repeat for the validation data\n",
    "\n",
    "X_valid = [X_valid.loc[patient].values for patient in veId]\n",
    "y_valid = [y_valid.loc[patient].values for patient in veId]\n",
    "\n",
    "X_valid = sequence.pad_sequences(X_valid, dtype='float32', maxlen=maxlen, padding='post', truncating='post')\n",
    "y_valid = sequence.pad_sequences(y_valid, dtype='float32', maxlen=maxlen, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, a lot just happened here:  \n",
    "\n",
    "1. We converted the _pandas_ DataFrame into a Python list, which contained lists of values (a list of list of values).  \n",
    "2. Using  `keras.preprocessing.sequence.pad_sequences`, we converted the value lists into a `numpy.array` of type `float32`, having a maximum length of 500.  \n",
    "3. If the patient encounter didn't have 500 encounters (most don't, see previous histogram) then we apply `padding='post'`, which says to pad with zeros.  That is, add extra rows (observation vectors) of all zeros.\n",
    "4. The option `truncating='post'` just says: if there are more than 500 observations, then take the first 500 and drop everything after.  \n",
    "\n",
    "Together, this says: **force patient encounter records of dimension 500x265 and use zero padding to inflate the size if needed**.  We could do something similar in _pandas_ but not as succinctly. This only requires a single command using _NumPy_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the shape of the array which will be used by the network\n",
    "# the shape is of the form (# of encounters, length of sequence, # of features)\n",
    "print(\"X_train shape: %s | y_train shape: %s\" % (str(X_train.shape), str(y_train.shape)))\n",
    "print(\"X_valid shape: %s | y_valid shape: %s\" % (str(X_valid.shape), str(y_valid.shape)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the type of X_train has changed to ```numpy.ndarray```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can plot the full patient encounter as a matrix plot.  Try a few times to get a feel for what the charts look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure out how many encounters we have\n",
    "numencnt = X_train.shape[0]\n",
    "\n",
    "# choose a random patient encounter to plot\n",
    "ix = random.randint(0,5000) #Try a few different index values between 0 and 4999\n",
    "print('ix = {}'.format(ix))\n",
    "\n",
    "# plot a matrix of observation values\n",
    "plt.title(\"Patient Encounter Matrix\")\n",
    "plt.pcolor(np.transpose(X_train[ix,:,:]))\n",
    "plt.ylabel(\"variable\")\n",
    "plt.xlabel(\"time/observation\")\n",
    "plt.ylim(0,265)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind here that these data are zero padded by ```sequence.pad_sequences```.  Try a few different index values between 0 and 4999 to get a feel for what the matrix plots look like. These matricies are what will be fed as input into the LSTM model for training. Notice that we can plot a variable in a similar fashion by selecting along the third axis instead of the first axis. This provides a view of a particular variable over all patient encounters. Give it a try!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='02_ex_pad'></a>\n",
    "### 2.3.1 Exercise: Padded Variable over all Patient Encounters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize a particular variable across all encounters with the following steps.  You can also check the [solution](#02_sol_pad) if you wish.\n",
    "1. Choose a variable to examine from [ehr_features.csv](csv/ehr_features.csv)\n",
    "2. Create a matrix of encounters vs time/observation\n",
    "3. Plot the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Step 1 set the variable value (feature number)\n",
    "varnum = None  #FIXME\n",
    "\n",
    "# TODO Step 2 Create a matrix of encounters vs time/observation\n",
    "# Hint: Select along the 3rd axis\n",
    "varmatrix = []  #FIXME\n",
    "\n",
    "# Step 3 Plot the matrix\n",
    "try:\n",
    "    plt.title(\"Variable Matrix\")\n",
    "    plt.pcolor(varmatrix) \n",
    "    plt.ylabel(\"time/observation\")\n",
    "    plt.xlabel(\"encounter\")\n",
    "    plt.ylim(0,600)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print('ERROR found: {}'.format(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"02_save\"></a>\n",
    "## 2.4 Save a *NumPy* Arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving to the next notebook, save the *NumPy* arrays to reload into the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the prepared numpy arrays for use in other notebooks\n",
    "np.save('X_train_prepared.npy',X_train,allow_pickle=False)\n",
    "np.save('y_train_prepared.npy',y_train,allow_pickle=False)\n",
    "np.save('X_valid_prepared.npy',X_valid,allow_pickle=False)\n",
    "np.save('y_valid_prepared.npy',y_valid,allow_pickle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green' size=10>Excellent work!</font><br>\n",
    "Now that we've prepared the data, we'll build and train our model.  Move on to the [next notebook](03_BuildModel.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/DLI Header.png\" alt=\"Header\" style=\"width: 400px;\"/>\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='02_solutions'></a>\n",
    "# Solutions\n",
    "<a name='02_sol_pad'></a>\n",
    "### 2.3.1 Exercise: Padded Variable over all Patient Encounters \n",
    "[Jump back to Exercise 2.3.1](#02_ex_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Step 1 set the variable value (feature number)\n",
    "varnum = 115 #FIXME  (any number between 0 and 264, inclusive)\n",
    "\n",
    "# TODO Step 2 Create a matrix of encounters vs time/observation\n",
    "# Hint: Select along the 3rd axis\n",
    "varmatrix = np.transpose(X_train[:,:,varnum])  #FIXME\n",
    "\n",
    "# Step 3 Plot the matrix\n",
    "try:\n",
    "    plt.title(\"Variable Matrix\")\n",
    "    plt.pcolor(varmatrix) \n",
    "    plt.ylabel(\"time/observation\")\n",
    "    plt.xlabel(\"encounter\")\n",
    "    plt.ylim(0,600)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print('ERROR found: {}'.format(e))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
